{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 4229472,
          "sourceType": "datasetVersion",
          "datasetId": 2492800
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "CogniTrack-Alzheimer Detection ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diviiij/CogniTrack/blob/main/CogniTrack_Alzheimer_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "uraninjo_augmented_alzheimer_mri_dataset_path = kagglehub.dataset_download('uraninjo/augmented-alzheimer-mri-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "2dOH0YpCDCvr"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:44:19.095142Z",
          "iopub.execute_input": "2025-04-16T04:44:19.095414Z",
          "iopub.status.idle": "2025-04-16T04:44:57.051654Z",
          "shell.execute_reply.started": "2025-04-16T04:44:19.095394Z",
          "shell.execute_reply": "2025-04-16T04:44:57.050866Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "IITOHXTPDCvy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " # Standard libraries\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch and Torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# For splitting dataset and evaluation\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:45:38.594246Z",
          "iopub.execute_input": "2025-04-16T04:45:38.594722Z",
          "iopub.status.idle": "2025-04-16T04:45:38.599165Z",
          "shell.execute_reply.started": "2025-04-16T04:45:38.594694Z",
          "shell.execute_reply": "2025-04-16T04:45:38.598389Z"
        },
        "id": "L2L8APR8DCvz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = {\n",
        "    \"Mild\" : \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset/MildDemented \",\n",
        "    \"Moderate\" :\"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset/ModerateDemented\",\n",
        "    \"Non\" : \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset/NonDemented\",\n",
        "    \"VeryMild\"  :  \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset/VeryMildDemented\"\n",
        "}\n",
        "\n",
        "label_map = {\n",
        "    \"Mild\": 0,\n",
        "    \"Moderate\": 1,\n",
        "    \"Non\": 2,\n",
        "    \"VeryMild\": 3\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:45:41.294776Z",
          "iopub.execute_input": "2025-04-16T04:45:41.295055Z",
          "iopub.status.idle": "2025-04-16T04:45:41.299054Z",
          "shell.execute_reply.started": "2025-04-16T04:45:41.295036Z",
          "shell.execute_reply": "2025-04-16T04:45:41.298124Z"
        },
        "id": "Wu1MU9zRDCv0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " transform = transforms.Compose([\n",
        "     transforms.Resize((224,224)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.485,0.456,0.406],\n",
        "                             std=[0.229,0.224,0.225])\n",
        " ])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:45:43.360139Z",
          "iopub.execute_input": "2025-04-16T04:45:43.360421Z",
          "iopub.status.idle": "2025-04-16T04:45:43.364846Z",
          "shell.execute_reply.started": "2025-04-16T04:45:43.3604Z",
          "shell.execute_reply": "2025-04-16T04:45:43.363927Z"
        },
        "id": "v7xYhN2yDCv1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "class AlzMergeDataset(Dataset):\n",
        "        def __init__(self,dataset_paths,label_map,transform= None):\n",
        "            self.data =[]\n",
        "            self.labels =[]\n",
        "            self.transform = transform\n",
        "\n",
        "            for stage,path in dataset_paths.items():\n",
        "                    label = label_map[stage]\n",
        "                     # You can adjust the glob pattern depending on your folder structure\n",
        "                    image_paths = glob.glob(os.path.join(path, '**', '*.jpg'), recursive=True)\n",
        "                    for img_path in image_paths:\n",
        "                        self.data.append(img_path)\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            img_path = self.data[idx]\n",
        "            label = self.labels[idx]\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:45:44.73311Z",
          "iopub.execute_input": "2025-04-16T04:45:44.733391Z",
          "iopub.status.idle": "2025-04-16T04:45:44.739594Z",
          "shell.execute_reply.started": "2025-04-16T04:45:44.73337Z",
          "shell.execute_reply": "2025-04-16T04:45:44.738748Z"
        },
        "id": "hSgBNQn4DCv2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = AlzMergeDataset(dataset_paths, label_map, transform=transform)\n",
        "\n",
        "print(\"Total images:\", len(combined_dataset))\n",
        "print(\"Example:\", combined_dataset[0])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:45:55.254211Z",
          "iopub.execute_input": "2025-04-16T04:45:55.254513Z",
          "iopub.status.idle": "2025-04-16T04:46:02.687824Z",
          "shell.execute_reply.started": "2025-04-16T04:45:55.254488Z",
          "shell.execute_reply": "2025-04-16T04:46:02.687057Z"
        },
        "id": "oyh3BAi2DCv3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Get all indices and labels\n",
        "all_indices = list(range(len(combined_dataset)))\n",
        "all_labels = combined_dataset.labels  # We assigned this during dataset creation\n",
        "\n",
        "# Stratified split\n",
        "train_idx, val_idx = train_test_split(\n",
        "    all_indices,\n",
        "    test_size=0.2,\n",
        "    stratify=all_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create subsets\n",
        "train_dataset = Subset(combined_dataset, train_idx)\n",
        "val_dataset = Subset(combined_dataset, val_idx)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:46:08.231435Z",
          "iopub.execute_input": "2025-04-16T04:46:08.231799Z",
          "iopub.status.idle": "2025-04-16T04:46:08.258204Z",
          "shell.execute_reply.started": "2025-04-16T04:46:08.231772Z",
          "shell.execute_reply": "2025-04-16T04:46:08.257364Z"
        },
        "id": "rZjXJQjiDCv4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "train_labels = [combined_dataset.labels[i] for i in train_idx]\n",
        "val_labels = [combined_dataset.labels[i] for i in val_idx]\n",
        "\n",
        "print(\"Train distribution:\", Counter(train_labels))\n",
        "print(\"Val distribution:\", Counter(val_labels))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:46:11.224868Z",
          "iopub.execute_input": "2025-04-16T04:46:11.225168Z",
          "iopub.status.idle": "2025-04-16T04:46:11.235169Z",
          "shell.execute_reply.started": "2025-04-16T04:46:11.225143Z",
          "shell.execute_reply": "2025-04-16T04:46:11.234269Z"
        },
        "id": "pWscNFGEDCv5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:46:12.867998Z",
          "iopub.execute_input": "2025-04-16T04:46:12.868283Z",
          "iopub.status.idle": "2025-04-16T04:46:12.872568Z",
          "shell.execute_reply.started": "2025-04-16T04:46:12.868262Z",
          "shell.execute_reply": "2025-04-16T04:46:12.87183Z"
        },
        "id": "aL6xb1tLDCwe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/timm/densenet121.ra_in1k.git"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T13:47:54.516717Z",
          "iopub.execute_input": "2025-04-08T13:47:54.517034Z",
          "iopub.status.idle": "2025-04-08T13:47:57.482859Z",
          "shell.execute_reply.started": "2025-04-08T13:47:54.517006Z",
          "shell.execute_reply": "2025-04-08T13:47:57.48204Z"
        },
        "id": "NgAdPu0jDCwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " import torch\n",
        "from torchvision import models\n",
        "\n",
        "# Load base DenseNet121 model\n",
        "model = models.densenet121()\n",
        "\n",
        "# Load weights\n",
        "state_dict = torch.load(\"densenet121.ra_in1k/pytorch_model.bin\", map_location=\"cpu\")\n",
        "\n",
        "# Remove any prefix like 'model.' if needed\n",
        "# Example: state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
        "\n",
        "# Load the weights (ignore unexpected keys if any)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# Modify classifier for 4 classes (Alzheimer's stages)\n",
        "model.classifier = torch.nn.Linear(model.classifier.in_features, 4)\n",
        "\n",
        "# Move to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"Model loaded and ready! üß†üí•\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:46:59.511808Z",
          "iopub.execute_input": "2025-04-16T04:46:59.512096Z",
          "iopub.status.idle": "2025-04-16T04:46:59.687115Z",
          "shell.execute_reply.started": "2025-04-16T04:46:59.512076Z",
          "shell.execute_reply": "2025-04-16T04:46:59.68597Z"
        },
        "id": "NQL1WPnADCwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files and folders inside the cloned repo\n",
        "for root, dirs, files in os.walk(\"densenet121.ra_in1k\"):\n",
        "    for name in files:\n",
        "        print(os.path.join(root, name))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:04.383241Z",
          "iopub.execute_input": "2025-04-16T04:47:04.383561Z",
          "iopub.status.idle": "2025-04-16T04:47:04.387486Z",
          "shell.execute_reply.started": "2025-04-16T04:47:04.383532Z",
          "shell.execute_reply": "2025-04-16T04:47:04.38662Z"
        },
        "id": "X0ZpNlSxDCwg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Modify final classifier to fit 4 classes\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 4)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:09.397286Z",
          "iopub.execute_input": "2025-04-16T04:47:09.397569Z",
          "iopub.status.idle": "2025-04-16T04:47:09.401692Z",
          "shell.execute_reply.started": "2025-04-16T04:47:09.397548Z",
          "shell.execute_reply": "2025-04-16T04:47:09.400949Z"
        },
        "id": "Jcm3wDD4DCwg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:11.537276Z",
          "iopub.execute_input": "2025-04-16T04:47:11.53759Z",
          "iopub.status.idle": "2025-04-16T04:47:11.541497Z",
          "shell.execute_reply.started": "2025-04-16T04:47:11.537563Z",
          "shell.execute_reply": "2025-04-16T04:47:11.540592Z"
        },
        "id": "3FojKIaxDCwh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Dataloaders (replace with your own)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:13.263114Z",
          "iopub.execute_input": "2025-04-16T04:47:13.263459Z",
          "iopub.status.idle": "2025-04-16T04:47:13.571136Z",
          "shell.execute_reply.started": "2025-04-16T04:47:13.263429Z",
          "shell.execute_reply": "2025-04-16T04:47:13.570202Z"
        },
        "id": "pYB1tQuMDCwh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /kaggle/input\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T11:16:16.095655Z",
          "iopub.execute_input": "2025-04-08T11:16:16.096045Z",
          "iopub.status.idle": "2025-04-08T11:16:16.250372Z",
          "shell.execute_reply.started": "2025-04-08T11:16:16.096023Z",
          "shell.execute_reply": "2025-04-08T11:16:16.249526Z"
        },
        "id": "4gTacgs6DCwh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Show the top-level structure\n",
        "print(os.listdir('/kaggle/input/augmented-alzheimer-mri-dataset'))\n",
        "\n",
        "# Try listing a subdirectory\n",
        "for subdir in os.listdir('/kaggle/input/augmented-alzheimer-mri-dataset'):\n",
        "    path = os.path.join('/kaggle/input/augmented-alzheimer-mri-dataset', subdir)\n",
        "    if os.path.isdir(path):\n",
        "        print(f\"\\nContents of '{subdir}':\")\n",
        "        print(os.listdir(path))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:18.565013Z",
          "iopub.execute_input": "2025-04-16T04:47:18.565354Z",
          "iopub.status.idle": "2025-04-16T04:47:18.575527Z",
          "shell.execute_reply.started": "2025-04-16T04:47:18.565324Z",
          "shell.execute_reply": "2025-04-16T04:47:18.574697Z"
        },
        "id": "p5T5hz01DCwh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:22.744062Z",
          "iopub.execute_input": "2025-04-16T04:47:22.744347Z",
          "iopub.status.idle": "2025-04-16T04:47:22.752349Z",
          "shell.execute_reply.started": "2025-04-16T04:47:22.744326Z",
          "shell.execute_reply": "2025-04-16T04:47:22.751503Z"
        },
        "id": "kQiUmxxPDCwh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
        "    print(root)\n",
        "    for d in dirs:\n",
        "        print(\"   üìÅ\", d)\n",
        "    for f in files:\n",
        "        print(\"   üìÑ\", f)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:38.444163Z",
          "iopub.execute_input": "2025-04-16T04:47:38.444442Z",
          "iopub.status.idle": "2025-04-16T04:47:40.287907Z",
          "shell.execute_reply.started": "2025-04-16T04:47:38.44442Z",
          "shell.execute_reply": "2025-04-16T04:47:40.286773Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "lV0XMnTRDCwi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "base_dir = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "work_dir = \"/kaggle/working/alzheimers_split\"\n",
        "\n",
        "# Create train and val folders\n",
        "for split in [\"train\", \"val\"]:\n",
        "    for class_name in os.listdir(base_dir):\n",
        "        os.makedirs(os.path.join(work_dir, split, class_name), exist_ok=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:45.280887Z",
          "iopub.execute_input": "2025-04-16T04:47:45.281164Z",
          "iopub.status.idle": "2025-04-16T04:47:45.288052Z",
          "shell.execute_reply.started": "2025-04-16T04:47:45.281143Z",
          "shell.execute_reply": "2025-04-16T04:47:45.287362Z"
        },
        "id": "S09Zaa_PDCwi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in os.listdir(base_dir):\n",
        "    class_dir = os.path.join(base_dir, class_name)\n",
        "    images = os.listdir(class_dir)\n",
        "    train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
        "\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(work_dir, \"train\", class_name, img))\n",
        "    for img in val_imgs:\n",
        "\n",
        "        shutil.copy(os.path.join(class_dir, img), os.path.join(work_dir, \"val\", class_name, img))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:47:46.88929Z",
          "iopub.execute_input": "2025-04-16T04:47:46.889603Z",
          "iopub.status.idle": "2025-04-16T04:48:07.273493Z",
          "shell.execute_reply.started": "2025-04-16T04:47:46.889576Z",
          "shell.execute_reply": "2025-04-16T04:48:07.272331Z"
        },
        "id": "xQcIxfo1DCwi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(os.path.join(work_dir, \"train\"), transform=transform)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(work_dir, \"val\"), transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:07.274058Z",
          "iopub.status.idle": "2025-04-16T04:48:07.274312Z",
          "shell.execute_reply": "2025-04-16T04:48:07.274206Z"
        },
        "id": "36kbBgHgDCwi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "\n",
        "# Use the most up-to-date pre-trained weights\n",
        "weights = DenseNet121_Weights.DEFAULT\n",
        "model = densenet121(weights=weights)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:10.010118Z",
          "iopub.execute_input": "2025-04-16T04:48:10.010391Z",
          "iopub.status.idle": "2025-04-16T04:48:10.534147Z",
          "shell.execute_reply.started": "2025-04-16T04:48:10.010371Z",
          "shell.execute_reply": "2025-04-16T04:48:10.533154Z"
        },
        "id": "E5nV-wceDCwj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:14.214274Z",
          "iopub.execute_input": "2025-04-16T04:48:14.214555Z",
          "iopub.status.idle": "2025-04-16T04:48:14.251305Z",
          "shell.execute_reply.started": "2025-04-16T04:48:14.214533Z",
          "shell.execute_reply": "2025-04-16T04:48:14.250637Z"
        },
        "id": "UldGIz4SDCwj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:17.794983Z",
          "iopub.execute_input": "2025-04-16T04:48:17.795253Z",
          "iopub.status.idle": "2025-04-16T04:48:17.801755Z",
          "shell.execute_reply.started": "2025-04-16T04:48:17.795232Z",
          "shell.execute_reply": "2025-04-16T04:48:17.800913Z"
        },
        "id": "GNJChRDnDCwj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=2, )\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:20.625444Z",
          "iopub.execute_input": "2025-04-16T04:48:20.625756Z",
          "iopub.status.idle": "2025-04-16T04:48:20.629803Z",
          "shell.execute_reply.started": "2025-04-16T04:48:20.625731Z",
          "shell.execute_reply": "2025-04-16T04:48:20.628946Z"
        },
        "id": "Zd4ZtBZLDCwj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None or val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:23.638418Z",
          "iopub.execute_input": "2025-04-16T04:48:23.63883Z",
          "iopub.status.idle": "2025-04-16T04:48:23.643599Z",
          "shell.execute_reply.started": "2025-04-16T04:48:23.638787Z",
          "shell.execute_reply": "2025-04-16T04:48:23.642853Z"
        },
        "id": "NMgqTsBHDCwj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, path='best_model.pth'):\n",
        "    torch.save(model.state_dict(), path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:25.236174Z",
          "iopub.execute_input": "2025-04-16T04:48:25.236492Z",
          "iopub.status.idle": "2025-04-16T04:48:25.240355Z",
          "shell.execute_reply.started": "2025-04-16T04:48:25.236464Z",
          "shell.execute_reply": "2025-04-16T04:48:25.239609Z"
        },
        "id": "ZlGO9Q7_DCwk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T13:51:42.819084Z",
          "iopub.execute_input": "2025-04-08T13:51:42.81955Z",
          "iopub.status.idle": "2025-04-08T13:59:06.057697Z",
          "shell.execute_reply.started": "2025-04-08T13:51:42.819513Z",
          "shell.execute_reply": "2025-04-08T13:59:06.056372Z"
        },
        "id": "mt4_JXg0DCwk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    early_stopping_counter = 0\n",
        "    patience = 5\n",
        "    best_loss = None\n",
        "\n",
        "    print(f\"Using: {device}\")\n",
        "\n",
        "    # Start the training timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Early stopping and checkpointing\n",
        "        if best_loss is None or val_loss <= best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(\"Model checkpoint saved.\")\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "            if early_stopping_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # End the training timer\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    minutes = int(elapsed_time // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "\n",
        "    print(f\"\\n‚úÖ Training completed in {minutes} minutes and {seconds} seconds.\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-08T14:00:14.60495Z",
          "iopub.execute_input": "2025-04-08T14:00:14.605332Z",
          "iopub.status.idle": "2025-04-08T14:00:14.615877Z",
          "shell.execute_reply.started": "2025-04-08T14:00:14.605301Z",
          "shell.execute_reply": "2025-04-08T14:00:14.614955Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "id": "9moGMZXzDCwk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.amp import autocast, GradScaler\n",
        "import time\n",
        "\n",
        "# ==== DEVICE SETUP ====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", device)\n",
        "\n",
        "# ==== TRANSFORMS ====\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# ==== DATASETS & LOADERS ====\n",
        "data_dir = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "dataset = ImageFolder(data_dir, transform=transform)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# Split manually\n",
        "indices = list(range(len(dataset)))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=[dataset.targets[i] for i in indices], random_state=42)\n",
        "train_dataset = Subset(dataset, train_idx)\n",
        "val_dataset = Subset(dataset, val_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# ==== MODEL ====\n",
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "model = densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, len(class_names))\n",
        "model = model.to(device)\n",
        "\n",
        "# ==== TRAINING SETUP ====\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# ==== EARLY STOPPING CLASS ====\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None or val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "# ==== MODEL CHECKPOINT ====\n",
        "def save_checkpoint(model, path=\"best_model.pth\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "# ==== TRAINING LOOP ====\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct / total * 100\n",
        "\n",
        "    # ==== VALIDATION ====\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total * 100\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "          f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save model if improved\n",
        "    if early_stopping.best_loss is None or val_loss < early_stopping.best_loss:\n",
        "        save_checkpoint(model)\n",
        "        print(\"Model checkpoint saved.\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "        pass\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "hours = int(elapsed_time // 3600)\n",
        "minutes = int((elapsed_time % 3600) // 60)\n",
        "seconds = int(elapsed_time % 60)\n",
        "\n",
        "print(f\"\\nTraining completed in {hours}h {minutes}m {seconds}s\")\n",
        "\n",
        "\n",
        "# ==== FINAL EVALUATION ====\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=class_names))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-16T04:48:42.312623Z",
          "iopub.execute_input": "2025-04-16T04:48:42.312938Z",
          "execution_failed": "2025-04-17T04:00:56.474Z"
        },
        "id": "QQNVhFDFDCwl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Subset, DataLoader\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-17T04:00:56.475Z"
        },
        "id": "Vp4qQnn5DCwm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_few_shot_task(dataset, n_classes=2, k_shot=5):\n",
        "    \"\"\"\n",
        "    Samples a task with n_classes, each with k_shot examples.\n",
        "    \"\"\"\n",
        "    class_indices = {}\n",
        "    for idx, (_, label) in enumerate(dataset):\n",
        "        class_indices.setdefault(label, []).append(idx)\n",
        "\n",
        "    selected_classes = random.sample(class_indices.keys(), n_classes)\n",
        "    selected_indices = []\n",
        "    for cls in selected_classes:\n",
        "        selected_indices += random.sample(class_indices[cls], k_shot)\n",
        "\n",
        "    return Subset(dataset, selected_indices)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-17T04:00:56.475Z"
        },
        "id": "D1BgFnqDDCwm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def reptile_train(model, train_dataset, meta_iterations=500, inner_steps=5, meta_lr=0.05, n_classes=2, k_shot=10):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    def inner_loop(cloned_model, task_loader):\n",
        "        optimizer = torch.optim.SGD(cloned_model.parameters(), lr=1e-2)\n",
        "        cloned_model.train()\n",
        "        for _ in range(inner_steps):\n",
        "            for images, labels in task_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "                    outputs = cloned_model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "    best_model_state = None\n",
        "    best_meta_loss = float(\"inf\")\n",
        "\n",
        "    print(\"Starting Reptile training...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for iteration in range(meta_iterations):\n",
        "        task_data = get_few_shot_task(train_dataset, n_classes=n_classes, k_shot=k_shot)\n",
        "        task_loader = DataLoader(task_data, batch_size=4, shuffle=True)\n",
        "\n",
        "        # Clone model\n",
        "        cloned_model = copy.deepcopy(model).to(device)\n",
        "\n",
        "        # Inner-loop task training\n",
        "        inner_loop(cloned_model, task_loader)\n",
        "\n",
        "        # Outer-loop Reptile update\n",
        "        for param, cloned_param in zip(model.parameters(), cloned_model.parameters()):\n",
        "            param.data = param.data + meta_lr * (cloned_param.data - param.data)\n",
        "\n",
        "        if (iteration + 1) % 50 == 0:\n",
        "            print(f\"[Meta-Iter {iteration + 1}] Reptile step complete.\")\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n‚úÖ Reptile training completed in {int(elapsed // 60)}m {int(elapsed % 60)}s.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-17T04:00:56.475Z"
        },
        "id": "VdwlaTIYDCwm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "reptile_train(model, train_dataset, meta_iterations=500, inner_steps=5, meta_lr=0.05, n_classes=2, k_shot=10)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-17T04:00:56.475Z"
        },
        "id": "bY_m0LEKDCwn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FQ13KZdLDCwn"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}